{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks with Cogito\n",
    "## 1 Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This course will teach you a about how we can create neural networks and use them to solve hard problems. A neural network solves problems by simulating how the brain works. We will start of with some basics about a single neuron!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Neuron is an electrically excitable cell that receives, processes, and transmits information through electrical and chemical signals. These signals between neurons occur via specialized connections called synapses. Neurons can connect to each other to form neural pathways, and neural circuits. Neurons are the primary components of the central nervous system, which includes the brain and spinal cord, and of the peripheral nervous system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/neuron.png \"neuron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical neuron consists of a cell body (soma), dendrites, and an axon. Axons connect to dendrites via synapses.\n",
    "When a neuron recieves an electro-chemical signal the signal is propagated from the dendritic input, through the cell body, and down the axon to other neurons.\n",
    "\n",
    "A neuron only fires if its input signal exceeds a certain amount (the threshold) in a short time period.\n",
    "\n",
    "Synapses vary in strength:\n",
    "* Good connections allowing a large signal\n",
    "* Slight connections allow only a weak signal\n",
    "* Synapses can either be an ampifier or an inhibitory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know a few things about a neuron, let's create a simple model of a single neuron! This is called a perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perceptron](Images/Perceptron.png \"perceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron has inputs, weights, an activation and an output. To calculate the perceptron's output we compute a weighted sum of the input signal and compare it to some treshold $\\theta$ (\"theta\"). If the input is smaller than the treshold $\\theta$, the output is -1, otherwise the output is 1.\n",
    "\n",
    "$$ X = \\sum_{i=1}^{n} x_iw_i $$\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{cases}\n",
    "+1 & \\text{if } X >= \\theta \\\\\n",
    "-1 & \\text{if } X < \\theta \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of activation function is called the sign function and is one of many activation functions we can use. The total output of our neuron/perceptron becomes:\n",
    "\n",
    "$$ Y = sign[\\sum_{i=1}^{n} x_iw_i - \\theta] $$\n",
    "\n",
    "The activation function is there to simulate the firing (activation) of the neuron when the total input to the neuron is great enough. If the neuron got enough input, we say it got activated. The actual activation of the neuron varies depending on the type of activation function we use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Activation\n",
    "In a neural network, we can use many different activation functions. A few choices are the step function, sigmoid and a linear function:\n",
    "\n",
    "$$ Y_{step} = \\begin{cases}\n",
    "1 & \\text{if } X >= 0 \\\\\n",
    "0 & \\text{if } X < 0 \\\\\n",
    "\\end{cases} $$\n",
    "\n",
    "$$ Y_{sigmoid} = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "$$ Y_{linear} = x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement some activation functions in Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Click inside this block and press Ctrl+Enter to run (or use the Run button in the toolbar)\n",
    "print(\"All good!\")\n",
    "# If everything went well, you should see the text All good! below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's implement the step function:\n",
    "\n",
    "$$ Y_{step} = \\begin{cases}\n",
    "1 & \\text{if } X >= 0 \\\\\n",
    "0 & \\text{if } X < 0 \\\\\n",
    "\\end{cases} $$\n",
    "\n",
    "![](Images/stepfunction.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the step activation function (≈ 1 line)\n",
    "def step(z):\n",
    "    ### CODE START HERE\n",
    "    a = None\n",
    "    ### CODE END HERE\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"step(-6):\", step(-6))\n",
    "print(\"step(0):\", step(0))\n",
    "print(\"step(18):\", step(18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> step(-6) </td>\n",
    "    <td> 0 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> step(0) </td>\n",
    "    <td> 1 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> step(18) </td>\n",
    "    <td> 1 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sign function is almost the same as the step function, try that one too:\n",
    "\n",
    "$$ Y_{sign} = \\begin{cases}\n",
    "1 & \\text{if } X >= 0 \\\\\n",
    "-1 & \\text{if } X < 0 \\\\\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the sign activation function (≈ 1 line)\n",
    "def sign(z):\n",
    "    ### CODE START HERE\n",
    "    a = None\n",
    "    ### CODE END HERE\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sign(-6):\", sign(-6))\n",
    "print(\"sign(0):\", sign(0))\n",
    "print(\"sign(18):\", sign(18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> sign(-6) </td>\n",
    "    <td> -1 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> sign(0) </td>\n",
    "    <td> 1 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> sign(18) </td>\n",
    "    <td> 1 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's implement sigmoid. Sigmoid is one of the most used activation functions in neural networks.\n",
    "\n",
    "$$ Y_{sigmoid} = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "![](Images/sigmoid.png)\n",
    "\n",
    "(Hint: The math library has a useful function `exp(x)` = $ e^x $)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the sigmoid activation function (≈ 1 line)\n",
    "def sigmoid(z):\n",
    "    ### CODE START HERE\n",
    "    s = None\n",
    "    ### CODE END HERE\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sigmoid(-2):\", sigmoid(-2))\n",
    "print(\"sigmoid(0):\", sigmoid(0))\n",
    "print(\"sigmoid(3):\", sigmoid(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> sigmoid(-2) </td>\n",
    "    <td> 0.11920292202211755 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> sigmoid(0) </td>\n",
    "    <td> 0.5 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> sigmoid(3) </td>\n",
    "    <td> 0.9525741268224334 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Weights\n",
    "When we create our neuron, we need some initial values for the weights (w in perceptron image above). They decide how much the neuron will \"listen\" to signals from the different input nodes. We usually want to initialize these with a random value in $[-0.5, 0.5]$. We usually represent this as a vector:\n",
    "\n",
    "$$ \\boldsymbol w = \\begin{bmatrix} random.uniform(-0.5, 0.5) \\\\ random.uniform(-0.5, 0.5) \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement initialize weights where n is \n",
    "# the number of different weights (≈ 1 line)\n",
    "def init_weights(n):\n",
    "    ### CODE START HERE\n",
    "    w = None\n",
    "    ### CODE END HERE\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(init_weights(5)):\", len(init_weights(5)))\n",
    "w = init_weights(1000000)\n",
    "mu = sum(w)/1000000\n",
    "print(\"expected value:\", mu, \"\\nis normal:\", str(-0.005 < mu or mu > 0.005))\n",
    "print(\"within value range:\", str(min(w) > -0.5 and max(w) < 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> len(init_weights(5) </td>\n",
    "    <td> 5 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> is normal </td>\n",
    "    <td> True </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> within value range </td>\n",
    "    <td> True </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Output\n",
    "\n",
    "Now it's time to get some output from our neuron. Remember our equations from earlier:\n",
    "\n",
    "$ z = \\sum_{i=1}^{n} x_iw_i $\n",
    "\n",
    "$ Y = activation[z- \\theta] $\n",
    "\n",
    "Here we need to remember to remove the threshold from the input to the neuron. I will get back to why and what the treshold is later!\n",
    "\n",
    "(Hint: you might find python's [zip()](https://docs.python.org/3.7/library/functions.html#zip) useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement predict (≈ 2 line)\n",
    "def predict(x, w, theta, activation):\n",
    "    ### CODE START HERE\n",
    "    z = None\n",
    "    y = None\n",
    "    ### CODE END HERE\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"predict([0, 0],[0.2, 0.2], 0.2, step):\", predict([0, 0],[0.2, 0.2], 0.2, step))\n",
    "print(\"predict([0, 1],[0.2, 0.2], 0.2, step):\", predict([0, 1],[0.2, 0.2], 0.2, step))\n",
    "print(\"predict([1, 0],[0.2, 0.2], 0.2, sign):\", predict([1, 0],[0.2, 0.2], 0.2, sign))\n",
    "print(\"predict([1, 1],[0.2, 0.2], 0.2, sigmoid):\", predict([1, 1],[0.2, 0.2], 0.2, sigmoid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> predict([0, 1],[0.2, 0.2], 0.4, step) </td>\n",
    "    <td> 0 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> predict([0, 1],[0.2, 0.2], 0.2, step) </td>\n",
    "    <td> 1 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> predict([0, 1],[0.2, 0.2], 0.2, sign) </td>\n",
    "    <td> 1 </td> \n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td> predict([1, 1],[0.2, 0.2], 0.2, sigmoid) </td>\n",
    "    <td> 0.549833997312478 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Learning\n",
    "\n",
    "Now that we have a perceptron with input, activation and weights we _can_ use our neuron, but there is a small problem. Since the weights are random we will get a random combination of the input as the sum. What we need is a nice way to learn what the weights should be!\n",
    "\n",
    "In machine learning and in neural networks, we (usually) learn from examples. That means we show the neural net an example of what we want it to learn, and then we correct it by saying if it got the example right or wrong. We can do the exact same thing with our neuron.\n",
    "\n",
    "$$ \\epsilon_{x_1} = Y_{expected} - Y_{actual} $$\n",
    "\n",
    "where $x_1$ is our first example. From this we can find the learning formula for the i-th weight:\n",
    "\n",
    "$$ w_i = w_i + \\alpha * x_i(example) * e(example) $$\n",
    "\n",
    "where the $\\alpha$ is what we call a learning rate. More on $\\alpha$ later.\n",
    "\n",
    "Let's start by implementing the error function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a function to calculate the error (≈ 1 line)\n",
    "def error(expected_y, y):\n",
    "    ### CODE START HERE\n",
    "    return None\n",
    "    ### CODE END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"error(1, 0):\", error(1, 0))\n",
    "print(\"error(1, -1):\", error(1, -1))\n",
    "print(\"error(0, 1):\", error(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> error(1, 0) </td>\n",
    "    <td> 1 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> error(1, -1) </td>\n",
    "    <td> 2 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> error(0, 1) </td>\n",
    "    <td> -1 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now implement the uptade weights function\n",
    "\n",
    "(Hint: The x and w in this function are the arrays with the inputs and weights respectivly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement update weights (≈ 1-2 lines)\n",
    "def update_weight(x, w, error, learning_rate):\n",
    "    ### CODE START HERE\n",
    "    w = None\n",
    "    ### CODE END HERE\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"update_weight([1, 0], [0.2, 0.2], 1, 0.1):\", update_weight([1, 0],[0.2, 0.2], 1, 0.1))\n",
    "print(\"update_weight([1, 0], [0.2, 0.2], -1, 0.2):\", update_weight([1, 0], [0.2, 0.2], -1, 0.2))\n",
    "print(\"update_weight([1, 0, 1], [0.2, 0.2, 0.1], -1, 0.1):\", update_weight([1, 0, 1], [0.2, 0.2, 0.1], -1, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> update_weight([1, 0],[0.2, 0.2], 1, 0.1)</td>\n",
    "    <td> [0.30000000000000004, 0.2] </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> update_weight([1, 0], [0.2, 0.2], -1, 0.2)</td>\n",
    "    <td> [0.0, 0.2] </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> update_weight([1, 0, 1], [0.2, 0.2, 0.1], -1, 0.1) </td>\n",
    "    <td> [0.1, 0.2, 0.0] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Perceptron \n",
    "Now it's time to put it all together and create a learning perceptron! We are going to use our perceptron to learn the **and**, **or** and **xor** functions. For those unfamilliar with these logic operators, here's a short recap:\n",
    "\n",
    "> And, or and xor are logic operators. You give them two statemens and get back a true or a false. An example of the and operator can be: \"It is raining `and` I am wet\". This is only true if it is both raining and I am wet. The `or` operator is true if it's raining *or* I am wet *or* both. The `xor` (\"exlusive or\") is true only if _either_ it is raining *or* if I'm wet, but not if both!\n",
    "> \n",
    "> If that was a bit hairy, this truth table might help:\n",
    "![](Images/truth-table-and-or-xor.png)\n",
    "\n",
    "But before we can learn anything we need to put everything together. Let's finish up the perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function perceptron witch takes in our input X, expected output Y, activation, number of epochs, learning rate, treshold and the weights. Here the X is a matrix, not a vector. This is because it lets us represent all the training examples in one structure. \n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1 \n",
    "\\end{bmatrix} , \n",
    "Y_{and} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "The $Y$ is a vector with the value of the truth table in the corresponding place to the examples in $X$. *num_epocs* is the way we in machine learning say how many times we want to iterate over the entire training set(all our examples, more on this later). We also pass in the weights, so that we can initialize the perceptron with weights we have trained before.\n",
    "\n",
    "(Hint: Scroll up to see the functions we defined earlier, and try to see where they fit in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the perceptron\n",
    "def perceptron(X, Y, activation, num_ephocs=5, learning_rate=0.1, threshold = 0.1, w=None, printer=False):\n",
    "    \n",
    "    # initialize the weights\n",
    "    if w == None:\n",
    "        ### CODE START HERE (≈ 1 line)\n",
    "        w = None\n",
    "        ### CODE END HERE\n",
    "        \n",
    "    # iterate through the entire training set multiple times to learn\n",
    "    for epoc in range(num_ephocs):\n",
    "        ### CODE START HERE (≈ 4 lines)\n",
    "        # for every example in the trainingsett\n",
    "        for None in None:\n",
    "            \n",
    "            # calculate the predicted value\n",
    "            y_pred = None\n",
    "            \n",
    "            # find the error\n",
    "            err = None\n",
    "            \n",
    "            # update the weights\n",
    "            w = None\n",
    "        \n",
    "        ### CODE END HERE\n",
    "        \n",
    "        if printer:\n",
    "            pred = \"\"\n",
    "            for x in X:\n",
    "                pred += str(predict(x, w, threshold, activation)) + \",\"\n",
    "            print(\"Epoch:\", epoc + 1)\n",
    "            print(\"Prediction\\t [\", pred[:-1] , \"]\", sep=\"\")\n",
    "            print(\"Weights\\t\\t\", w, end=\"\\n\\n\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our perceptron returns its weights, we can use our predict method from earlier to test if our implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the X matrix\n",
    "X = [[0,0],\n",
    "     [0,1],\n",
    "     [1,0],\n",
    "     [1,1]]\n",
    "\n",
    "# set up the different Y vectors\n",
    "and_Y = [0, 0, 0, 1]\n",
    "or_Y = [0, 1, 1, 1]\n",
    "xor_Y = [0, 1, 1, 0]\n",
    "\n",
    "# threshold\n",
    "threshold = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run our perceptron and evaluate the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "and_weight = perceptron(X, and_Y, step, num_ephocs=10, threshold=threshold, printer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the printout that it learns the **and** weights in just a few iterations! Let's see if we can learn **or**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "or_weight = perceptron(X, or_Y, step,num_ephocs=7, threshold=threshold, printer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **or** weights are also easy to learn, but pay extra attention to the **xor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xor_weight = perceptron(X, xor_Y, step, num_ephocs=10, threshold=threshold, printer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with 10 steps it gets nowhere close to the answer and gets stuck after a few iterations. This is because it actually can't learn the **xor**! To understand why, we need to take a look at what the perceptron actually does when we train the weights and a little somthing called linear seperability.\n",
    "\n",
    "When we train our perceptron and estimate the weights, it can be shown that what we actually do is to fit a line (or hyperplane) through a plane (or whatever many dimensions each training example has).\n",
    "\n",
    "$$ x_1 w_1 + x_2 w_2 = \\theta $$\n",
    "\n",
    "We can then use this equation to calculate what side of the line something is. When we do this in the **and** case it looks somthing like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(and_weight)\n",
    "\n",
    "plt.plot([0, 0, 1], [0, 1, 0], \"ro\", markersize=20)\n",
    "plt.plot([1], [1], \"b+\", markersize=20)\n",
    "intercept = threshold/ and_weight[1]\n",
    "slope = (-and_weight[0]/and_weight[1])\n",
    "\n",
    "\n",
    "\n",
    "x = [intercept + (slope * x) for x in range(-3, 3)]\n",
    "\n",
    "plt.plot(x, range(-3, 3))\n",
    "plt.axis([-0.2, 1.2, -0.2, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have marked the positive case (`1 and 1 = 1`) as a plus sign and the negative cases as red dots. If you did everything correctly so far, you should se that the line seperates the plus from the red dots. This is what the perceptron does: it finds a line that seperates two different types of data. Now for the **or** case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(or_weight)\n",
    "\n",
    "plt.plot([0], [0], \"ro\", markersize=20)\n",
    "plt.plot([0, 1, 1], [1, 0, 1], \"b+\", markersize=20)\n",
    "intercept = threshold/ or_weight[1]\n",
    "slope = (-or_weight[0]/or_weight[1])\n",
    "\n",
    "x = [intercept + (slope * x) for x in range(-3, 3)]\n",
    "\n",
    "plt.plot(x, range(-3, 3))\n",
    "plt.axis([-0.2, 1.2, -0.2, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again the + sign are positive cases and the red dot is the negative case. We see that the same perceptron as in the and case has managed to learn a new line that separates the two clases. Let's se why the perceptron is not able to learn the **xor** case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(xor_weight)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"ro\", markersize=20)\n",
    "plt.plot([0, 1], [1, 0], \"b+\", markersize=20)\n",
    "intercept = 0.2/ xor_weight[1]\n",
    "slope = (-xor_weight[0]/ xor_weight[1])\n",
    "\n",
    "x = [intercept + (slope * x) for x in range(-5, 5)]\n",
    "\n",
    "plt.plot(x, range(-5, 5))\n",
    "plt.axis([-1.5, 2.5, -1.5, 2.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, now its apparent that the perceptron, aka. our \"line learner\", is not able to learn a line that seperates the two clases. If you manage to draw a straight line that solves the xor case, please come show me!\n",
    "\n",
    "Let's see if we can solve this problem by adding more neurons! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed I have been sneaking increasingly more and more vectors and matrices into our code. I'm not doing this just to mess with you, but rather to prepare you for the truth: \n",
    "\n",
    "We are almost always dealing with matrices and vectors when we work with neural networks. This will not only make your code look better, but we also get more efficient code in the process. We are now going to start playing with conecting multiple perceptrons together and training them with somthing called backpropagation. When we do that I'm going to start using some libraries that help us with the martix stuff, so don't worry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra task:\n",
    "If you have more time, try to run the perceptron one step at a time and plot every iteration, then you can see that the line gets closer and closer to the answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
